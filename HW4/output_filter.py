import argparse
import csv
import math
import os
import sys
from typing import Dict, List, Optional, Sequence

# Increase CSV field size limit to handle large position arrays
csv.field_size_limit(sys.maxsize)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description=(
            "Filter Homework4 output_data files and keep only the loads that "
            "successfully reached the steady-state end condition."
        )
    )
    parser.add_argument(
        "input_file",
        help="Path to the output_data_*.txt file generated by Homework4.py",
    )
    parser.add_argument(
        "-o",
        "--output-file",
        dest="output_file",
        help=(
            "Destination file for the filtered data. "
            "Defaults to '<input>_completed.txt'."
        ),
    )
    parser.add_argument(
        "--parameters",
        default="HW4/springNetworkData/parameters.txt",
        help=(
            "Path to the parameters.txt file. "
            "Used to compute the steady-state time window."
        ),
    )
    parser.add_argument(
        "--steady-time",
        type=float,
        default=None,
        help="Override steady-state time (seconds). If omitted, it is computed "
             "from the parameters file as 5 * (L / sqrt(E / rho)).",
    )
    parser.add_argument(
        "--steady-time-factor",
        type=float,
        default=5.0,
        help=(
            "Multiplier applied to the time scale when computing steady time "
            "from the parameters file. Default matches Homework4.py (5.0)."
        ),
    )
    parser.add_argument(
        "--time-reset-tol",
        type=float,
        default=1e-12,
        help="Tolerance used to detect when the simulation time resets (s).",
    )
    parser.add_argument(
        "--summary-file",
        dest="summary_file",
        help=(
            "Optional path for the attempted-time-step summary CSV. "
            "Defaults to '<input>_summary.csv'."
        ),
    )
    return parser.parse_args()


def load_parameters(path: str) -> Dict[str, float]:
    params: Dict[str, float] = {}
    if not os.path.exists(path):
        raise FileNotFoundError(
            f"Parameters file '{path}' not found. Provide --steady-time instead."
        )

    with open(path, "r", encoding="utf-8") as handle:
        for line in handle:
            if ":" not in line:
                continue
            key, value = [part.strip() for part in line.split(":", 1)]
            try:
                params[key] = float(value)
            except ValueError:
                continue
    return params


def compute_steady_time(args: argparse.Namespace) -> float:
    if args.steady_time is not None:
        return args.steady_time

    params = load_parameters(args.parameters)
    required = ("L", "E", "rho")
    missing = [p for p in required if p not in params]
    if missing:
        raise KeyError(
            f"Missing parameters {missing} in '{args.parameters}'. "
            "Either update the file or pass --steady-time."
        )

    L = params["L"]
    E = params["E"]
    rho = params["rho"]
    time_scale = L / math.sqrt(E / rho)
    return args.steady_time_factor * time_scale


def floats_equal(a: float, b: float, *, rel_tol: float = 1e-12, abs_tol: float = 0.0) -> bool:
    return math.isclose(a, b, rel_tol=rel_tol, abs_tol=abs_tol)


def format_float(value: Optional[float]) -> str:
    if value is None or (isinstance(value, float) and math.isnan(value)):
        return ""
    return f"{value:.12g}"


def get_time_step(row: Dict[str, str]) -> Optional[float]:
    for key in ("Time Step", "TimeStep", "dt"):
        if key in row and row[key]:
            try:
                return float(row[key])
            except ValueError:
                continue
    return None


def split_segments(
    rows: Sequence[Dict[str, str]],
    reset_tol: float,
) -> List[List[Dict[str, str]]]:
    """
    Split a load's rows into segments. Each segment corresponds to a single
    time-step attempt (ct resets to ~0 when Newton-Raphson fails and dt
    increases).
    """
    segments: List[List[Dict[str, str]]] = []
    current: List[Dict[str, str]] = []
    prev_time: Optional[float] = None

    for row in rows:
        time_val: Optional[float] = None
        time_str = row.get("Time")
        if time_str not in (None, ""):
            try:
                time_val = float(time_str)
            except ValueError:
                time_val = None

        if (
            current
            and time_val is not None
            and prev_time is not None
            and time_val < prev_time - reset_tol
        ):
            segments.append(current)
            current = []

        current.append(row)
        if time_val is not None:
            prev_time = time_val

    if current:
        segments.append(current)
    return segments


def analyze_segments(
    rows: Sequence[Dict[str, str]],
    load_value: Optional[float],
    steady_time: float,
    reset_tol: float,
) -> (Optional[List[Dict[str, str]]], List[Dict[str, str]]):
    """
    Inspect each time-step attempt (segment) for a load, return the one that
    satisfies the steady-state duration (if any) plus per-segment summary info.
    
    IMPORTANT: Only the LAST segment can be the successful one, because
    Homework4.py stops and moves to the next load once steady state is reached.
    All earlier segments are failed attempts (Newton-Raphson didn't converge).
    """
    segments = split_segments(rows, reset_tol)
    chosen: Optional[List[Dict[str, str]]] = None
    summary_entries: List[Dict[str, str]] = []

    # Only the LAST segment can be the successful one, because Homework4.py
    # stops and moves to the next load once steady state is reached.
    # All earlier segments are failed attempts (Newton-Raphson didn't converge).
    last_segment_idx = len(segments) - 1

    for attempt_idx, segment in enumerate(segments, start=1):
        max_time = 0.0
        time_values: List[float] = []
        for row in segment:
            time_str = row.get("Time")
            if time_str in (None, ""):
                continue
            try:
                time_val = float(time_str)
            except ValueError:
                continue
            max_time = max(max_time, time_val)
            time_values.append(time_val)

        dt = get_time_step(segment[-1]) or 0.0
        tolerance = dt if dt > 0 else reset_tol
        
        # Only the last segment can pass - earlier ones are failed attempts
        is_last_segment = (attempt_idx - 1) == last_segment_idx
        passed = is_last_segment and (max_time + tolerance >= steady_time)

        summary_entries.append(
            {
                "Load": format_float(load_value),
                "Attempt": str(attempt_idx),
                "Time Step": format_float(dt),
                "Max Time": format_float(max_time),
                "Time Steps": str(len(time_values)),
                "Rows": str(len(segment)),
                "Status": "passed" if passed else "failed",
            }
        )

        if passed:
            chosen = segment

    return chosen, summary_entries


def write_rows(
    writer: csv.DictWriter,
    rows: Sequence[Dict[str, str]],
) -> None:
    for row in rows:
        writer.writerow(row)


def main() -> None:
    args = parse_args()
    steady_time = compute_steady_time(args)

    input_path = args.input_file
    if not os.path.exists(input_path):
        raise FileNotFoundError(f"Input file '{input_path}' not found.")

    output_path = (
        args.output_file
        if args.output_file
        else f"{os.path.splitext(input_path)[0]}_completed.txt"
    )
    summary_path = (
        args.summary_file
        if getattr(args, "summary_file", None)
        else f"{os.path.splitext(input_path)[0]}_summary.csv"
    )

    completed_loads: List[float] = []
    discarded_loads: List[float] = []
    summary_rows: List[Dict[str, str]] = []

    with open(input_path, "r", encoding="utf-8") as src:
        reader = csv.DictReader(src)
        if not reader.fieldnames:
            raise ValueError("Input file has no header row.")

        with open(output_path, "w", newline="", encoding="utf-8") as dst:
            writer = csv.DictWriter(dst, fieldnames=reader.fieldnames)
            writer.writeheader()

            current_load: Optional[float] = None
            current_rows: List[Dict[str, str]] = []

            for row in reader:
                load_str = row.get("Load")
                if not load_str:
                    continue
                try:
                    load_val = float(load_str)
                except ValueError:
                    continue

                if current_load is None:
                    current_load = load_val
                elif not floats_equal(load_val, current_load):
                    selected_segment, segment_summaries = analyze_segments(
                        current_rows, current_load, steady_time, args.time_reset_tol
                    )
                    summary_rows.extend(segment_summaries)

                    if selected_segment:
                        write_rows(writer, selected_segment)
                        completed_loads.append(current_load)
                    else:
                        discarded_loads.append(current_load)
                    current_rows = []
                    current_load = load_val

                current_rows.append(row)

            # Flush final group
            if current_rows:
                selected_segment, segment_summaries = analyze_segments(
                    current_rows, current_load, steady_time, args.time_reset_tol
                )
                summary_rows.extend(segment_summaries)

                if selected_segment:
                    write_rows(writer, selected_segment)
                    if current_load is not None:
                        completed_loads.append(current_load)
                else:
                    if current_load is not None:
                        discarded_loads.append(current_load)

    # Write summary file
    summary_fields = [
        "Load",
        "Attempt",
        "Time Step",
        "Max Time",
        "Time Steps",
        "Rows",
        "Status",
    ]
    with open(summary_path, "w", newline="", encoding="utf-8") as summary_file:
        summary_writer = csv.DictWriter(summary_file, fieldnames=summary_fields)
        summary_writer.writeheader()
        for row in summary_rows:
            summary_writer.writerow(row)

    print(f"Wrote filtered data to: {output_path}")
    print(f"Wrote summary data to:  {summary_path}")
    print(f"Completed loads: {len(completed_loads)}")
    if completed_loads:
        print("  " + ", ".join(f"{val:.6g}" for val in completed_loads))
    print(f"Discarded loads: {len(discarded_loads)}")
    if discarded_loads:
        print("  " + ", ".join(f"{val:.6g}" for val in discarded_loads))


def run_filter_direct(input_file: str, output_file: str = None, 
                     summary_file: str = None, parameters_file: str = 'HW4/springNetworkData/parameters.txt',
                     steady_time_factor: float = 4.0, time_reset_tol: float = 1e-12):
    """
    Run the filter directly with specified parameters.
    
    Parameters:
    -----------
    input_file : str
        Path to input file
    output_file : str, optional
        Path to output file (default: <input>_completed.txt)
    summary_file : str, optional
        Path to summary file (default: <input>_summary.csv)
    parameters_file : str, optional
        Path to parameters file
    steady_time_factor : float, optional
        Multiplier for steady time (default: 4.0 to match Homework4.py)
    time_reset_tol : float, optional
        Tolerance for detecting time resets
    """
    # Create a mock args object
    class Args:
        def __init__(self):
            self.input_file = input_file
            self.output_file = output_file
            self.summary_file = summary_file
            self.parameters = parameters_file
            self.steady_time = None
            self.steady_time_factor = steady_time_factor
            self.time_reset_tol = time_reset_tol
    
    args = Args()
    steady_time = compute_steady_time(args)
    
    output_path = (
        args.output_file
        if args.output_file
        else f"{os.path.splitext(input_file)[0]}_completed.txt"
    )
    summary_path = (
        args.summary_file
        if getattr(args, "summary_file", None)
        else f"{os.path.splitext(input_file)[0]}_summary.csv"
    )
    
    completed_loads: List[float] = []
    discarded_loads: List[float] = []
    summary_rows: List[Dict[str, str]] = []
    
    with open(input_file, "r", encoding="utf-8") as src:
        reader = csv.DictReader(src)
        if not reader.fieldnames:
            raise ValueError("Input file has no header row.")
    
        with open(output_path, "w", newline="", encoding="utf-8") as dst:
            writer = csv.DictWriter(dst, fieldnames=reader.fieldnames)
            writer.writeheader()
    
            current_load: Optional[float] = None
            current_rows: List[Dict[str, str]] = []
    
            for row in reader:
                load_str = row.get("Load")
                if not load_str:
                    continue
                try:
                    load_val = float(load_str)
                except ValueError:
                    continue
    
                if current_load is None:
                    current_load = load_val
                elif not floats_equal(load_val, current_load):
                    chosen, summary = analyze_segments(
                        current_rows, current_load, steady_time, args.time_reset_tol
                    )
                    summary_rows.extend(summary)
                    if chosen:
                        write_rows(writer, chosen)
                        completed_loads.append(current_load)
                    else:
                        discarded_loads.append(current_load)
                    current_load = load_val
                    current_rows = []
    
                current_rows.append(row)
    
            if current_load is not None:
                chosen, summary = analyze_segments(
                    current_rows, current_load, steady_time, args.time_reset_tol
                )
                summary_rows.extend(summary)
                if chosen:
                    write_rows(writer, chosen)
                    completed_loads.append(current_load)
                else:
                    discarded_loads.append(current_load)
    
    with open(summary_path, "w", newline="", encoding="utf-8") as summary_file:
        summary_fields = [
            "Load",
            "Attempt",
            "Time Step",
            "Max Time",
            "Time Steps",
            "Rows",
            "Status",
        ]
        summary_writer = csv.DictWriter(summary_file, fieldnames=summary_fields)
        summary_writer.writeheader()
        summary_writer.writerows(summary_rows)
    
    print(f"Wrote filtered data to: {output_path}")
    print(f"Wrote summary data to:  {summary_path}")
    print(f"Completed loads: {len(completed_loads)}")
    if completed_loads:
        print("  " + ", ".join(f"{val:.6g}" for val in completed_loads))
    print(f"Discarded loads: {len(discarded_loads)}")
    if discarded_loads:
        print("  " + ", ".join(f"{val:.6g}" for val in discarded_loads))


if __name__ == "__main__":
    # Set to True to run directly with hardcoded parameters, False to use command line
    RUN_DIRECTLY = True  # Change to False to use command line arguments
    
    if RUN_DIRECTLY:
        # Modify these parameters to run directly without command line arguments:
        run_filter_direct(
            input_file='HW4/plots/output_data_0.040m_20251124_171957.txt',
            output_file=None,  # Set to None for default: <input>_completed.txt
            summary_file=None,  # Set to None for default: <input>_summary.csv
            parameters_file='HW4/springNetworkData/parameters.txt',
            steady_time_factor=4.0,  # Match Homework4.py (4 * time_scale)
            time_reset_tol=1e-12
        )
    else:
        main()